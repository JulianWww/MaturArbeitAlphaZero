\title{Mastering the game of Connect 4 through self-play}
\author{
        Julian Wandhoven \\
                fgz\\
}
\date{\today}

\documentclass[12pt]{article}
\usepackage[dvipsnames]{xcolor}
\usepackage{amssymb}
\usepackage{tabularx}
\usepackage{titlesec}
\usepackage{amsmath}
\usepackage{svg}
\usepackage{caption}
\usepackage{hyperref}
\usepackage{lscape}
\usepackage{import}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{minted}

\usepackage{rotating}
\usepackage{tikz}

\usepackage{graphicx}
\graphicspath{ {./images/} }

\setcounter{secnumdepth}{5}
\setcounter{tocdepth}{5}
\lstset { %
    language=C++,
    backgroundcolor=\color{black!5}, % set backgroundcolor
    basicstyle=\footnotesize,% basic font setting
}



\titleformat{\paragraph}
{\normalfont\normalsize\bfseries}{\theparagraph}{1em}{}
\titlespacing*{\paragraph}
{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}

\titleformat{\subparagraph}
{\normalfont\normalsize\bfseries}{\thesubparagraph}{1em}{}
\titlespacing*{\subparagraph}
{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}

\newcommand{\imgRef}[1]{(fig \ref{#1} on page \pageref{#1})}
\newcommand{\equationref}[1]{equation \ref{#1} on page \pageref{#1}}
\newcommand{\sectionref}[1]{section \ref{#1} on page \pageref{#1}}
\newcommand{\listingref}[1]{listing \ref{#1} on page \pageref{#1}}
\newcommand{\source}[1]{\caption*{Source: {#1}} }
\newcommand{\quckeq}[1]{(\ref{#1})}

\newcommand{\tickTackToe}[9]{
\begin{tabular}{p{7px}|p{7px}|p{7px}}
\multicolumn{3}{c}{}\\
  #1 & #2 & #3 \\      \hline
  #4 & #5 & #6 \\      \hline
   & #7 & #8\\
\multicolumn{3}{c}{#9}
\end{tabular}
}
\newcommand{\ticTacToe}[9]{
\begin{tabular}{p{7px}|p{7px}|p{7px}}
  #1 & #2 & #3 \\      \hline
  #4 & #5 & #6 \\      \hline
  #7 & #8 & #9 \\
\end{tabular}
}
\newcommand{\rewardArrow}[1]{\(\xrightarrow[\text{#1}]{\text{reward}}\) }

\newcommand{\FittedEloRaiting}{408}
\newcommand{\FittedScoringThreshold}{2.8}

\newcommand{\mathColor}[2]{\color{#1}#2\color{black}}
\newcommand{\gold}{YellowOrange}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=blue,
    pdfpagemode=FullScreen,
    }

\urlstyle{same}

\definecolor{mygreen}{rgb}{0,0.6,0}
\definecolor{mygray}{rgb}{0.5,0.5,0.5}
\definecolor{mymauve}{rgb}{0.58,0.29,0}
\definecolor{codeLightGray}{rgb}{0.9,0.9,0.9}

\lstset{ 
  backgroundcolor=\color{codeLightGray},   % choose the background color; you must add \usepackage{color} or \usepackage{xcolor}; should come as last argument
  basicstyle=\footnotesize,        % the size of the fonts that are used for the code
  breakatwhitespace=false,         % sets if automatic breaks should only happen at whitespace
  breaklines=true,                 % sets automatic line breaking
  captionpos=b,                    % sets the caption-position to bottom
  commentstyle=\color{mygreen},    % comment style
  deletekeywords={...},            % if you want to delete keywords from the given language
  escapeinside={\%*}{*)},          % if you want to add LaTeX within your code
  extendedchars=true,              % lets you use non-ASCII characters; for 8-bits encodings only, does not work with UTF-8
  %frame=single,	                   % adds a frame around the code
  keepspaces=true,                 % keeps spaces in text, useful for keeping indentation of code (possibly needs columns=flexible)
  keywordstyle=\color{blue},       % keyword style
  language=C++,                 % the language of the code
  morekeywords={*,...},            % if you want to add more keywords to the set
  numbers=left,                    % where to put the line-numbers; possible values are (none, left, right)
  numbersep=5pt,                   % how far the line-numbers are from the code
  numberstyle=\tiny\color{mygray}, % the style that is used for the line-numbers
  rulecolor=\color{black},         % if not set, the frame-color may be changed on line-breaks within not-black text (e.g. comments (green here))
  showspaces=false,                % show spaces everywhere adding particular underscores; it overrides 'showstringspaces'
  showstringspaces=false,          % underline spaces within strings only
  showtabs=false,                  % show tabs within strings adding particular underscores
  stepnumber=1,                    % the step between two line-numbers. If it's 1, each line will be numbered
  stringstyle=\color{mymauve},     % string literal style
  tabsize=2,	                   % sets default tabsize to 2 spaces
  %title=\lstname                   % show the filename of files included with \lstinputlisting; also try caption instead of title
}
\newcommand{\incFile}[2]{\label{code:#2}\lstinputlisting[language=#1]{code/#2}\newpage}
\newcommand{\incDemo}[2]{\label{demo:#2}\lstinputlisting[language=#1]{Matura-AlphaZero-demos/#2}\newpage}

\begin{document}
\maketitle
\begin{abstract}
%\noindent A reimplementation of the Alpha Zero algorithm in C++ using the PyTorch library, based on the 2 papers by Google Deep Mind~\cite{silver2018general}\cite{silver2017mastering}.
\noindent Alpha Zero is an AI algorythem, that is capable of learning to play zero sum stated multiplayer games. These types of games include Go, Chess, Phi Sho and so forth. This is done by training a neural network and from data generated by a Monte Carlo Tree Serch. This document also explains how neural networks work and a short explenation of the infrastructure around the AI to allow for playing on remote devices.
~\cite{silver2018general}\cite{silver2017mastering}
\end{abstract}
\newpage
\tableofcontents
\newpage

Alpha Zero is an algorithm published in 2018 by Google Deepmind as the generalization of AlphaGo Zero, an algorithm that learned to play the game of Go using only the rules of the game. In the generalized version, the same principles were applied to Chess and Shogi. Unlike previous algorithms such as StockFish or Elmo that use hand-crafted evaluation functions along with alpha-beta searches over a large search space, Alpha Zero uses no human knowledge. Rather, it generates all information through self-play. It has been shown to achieve superhuman performance in Chess, Shogi and Go. In this project the AI will be trained to play connect4. The entire algorithm is implemented in C++ to increase efficiency during the Monte Carlo Tree Search (MCTS). Furthermore, to allow for better performance when playing, all computations are handled via a server (my desktop). A secondary server has also been added to allow for easy re-routing of the main server and handle things like elo-ratings (see \sectionref{sec:Evaluation:elo-rating}) for all agents. Additionally, I have added a short introduction on how neural networks work and how they are trained in \sectionref{NN}. 

\section{Methods}
\label{Methods}
The Alpha Zero algorithm is a reinforcement learning algorithm using two major parts: a) a {\it Monte Carlo tree search} (MCTS) that is guided by b) the {\it neural network} to improve performance.
The agent (computer player) runs a certain amount of simulation games using its MCTS and neural network. At each step, the MCTS evaluates the most promising next states as given by the neural network's estimation. The MCTS, by simulating games starting from the current state, will improve the neural network's prediction for that state. At the end of each game, the winner is determined and used to update the neural network's estimation of who would win a game starting from a certain state.
\subsection{Reinforcement Learning}
When training neural networks, there are three major possible situations: Supervised learning, unsupervised learning, and reinforcement learning. The first uses predetermined data with known in- and outputs the network is trained to predict. An example of supervised learning is the recognition of handwriting as the data is defined by humans. This method consists of creating a large database of examples, and the neural network is then trained to predict a given output for all examples. 

Unsupervised learning or self-organization is used when there is no previous available data and the neural network has to create those classifications itself. An example of unsupervised learning is Vector quantization. The algorythems sorts points in n-dimensional space into a predetermined amount of groups. Every group is defined by its centroid point. Training happens by selecting a sample point at random, and moving the closest centroid point towards the sample point by a fraction of the distance betwean them. The sample point is selected from the input data \cite{wiki:Vector_quantization}. An example of both supervised and unsupervised learning can be seen in the demo\footnote{demo is at \url{https://github.com/JulianWww/Matura-AlphaZero-demos}}.

These two methods represent the extreme ends of the spectrum. Reinforcement leaning on the other hand can be thought of as an intermediate form. It uses a predetermined environment which gives positive, neutral and negative feedback. The neural network is then discouraged from taking actions leading to negative feedback and encouraged to take actions leading to positive feedback. The feedback is determined by the environment the agent learns to interact with. In this case, losing a game would be bad and result in negative feedback whereas winning a game leads to positive feedback. Ties lead to neutral feedback. The agents learning is set up in such a way, that it is encouraged to take actions leading to positive feedback and discouraged from taking actions that lead to negative feedback. However actions, can lead to a loss that only occurs many game steps in the future. A common approach to solve this problem is to have the feedback propagate backwards to previous actions. In Alpha Zero, this is handled by the memory (see \sectionref{sec:memory}). When the game reaches an end state and a winner is determined, the feedback is propagated backwards up the game. If the player won, the feedback is positive. If he lost, it is negative. More specifically, if a player takes an action \(a_s\) at a state \(s\), that leads to a win, the reward for that state is defined as \(R(s,a_s) = 1\). On the other hand, if the action leads to a loss, the reward will be \(R(s, a_s) = -1\). If the game ends in a tie, the reward is \(R(s,a_s) = 0\). Every agent \(p\) will try to maximize \(\sum_{s\in g\cap p} R(s, a_s)\). \(g\cap p\) is the set of all states in which the player \(p\) takes an action. Let's look at a tic tac toe example of the following game:
\begin{center}
\tickTackToe{}{}{}{}{}{}{}{}{player X} \to \tickTackToe{X}{}{}{}{}{}{}{}{player O} \to \tickTackToe{X}{O}{}{}{}{}{}{}{player X} \to \tickTackToe{X}{O}{}{}{X}{}{}{}{player O} \to \tickTackToe{X}{O}{}{O}{X}{}{}{}{player X} \to \tickTackToe{X}{O}{}{O}{X}{}{}{X}{}
\end{center}
Since player \(X\) won the game, the reward for every state \(s\in g\cap X\) is \(R(s, a_s) = 1\) and the reward for every state \(s\in g\cap O\) is \(R(s, a_s) = -1\). The reward for the entire game is:
\begin{center}
\tickTackToe{}{}{}{}{}{}{}{}{} \rewardArrow{1} \tickTackToe{X}{}{}{}{}{}{}{}{} \rewardArrow{-1} \tickTackToe{X}{O}{}{}{}{}{}{}{} \rewardArrow{1} \tickTackToe{X}{O}{}{}{X}{}{}{}{} \rewardArrow{-1} \tickTackToe{X}{O}{}{O}{X}{}{}{}{} \rewardArrow{1} \tickTackToe{X}{O}{}{O}{X}{}{}{X}{}
\end{center}

The important thing to keep in mind is that reinforcement learning algorithms encourage actions that lead to a positive feedback and discourage actions that lead to a negative feedback.
\subsection{Game}
The game is the environment, that is used to train the AI. The game consists of constant unchanging game states. Every game state consists of a game board and a player. An end game state is a state at which the game is done. This means that one player won or the game ended in a tie. For connect4 this means four stones in a line or a full game board.
Let \(\mathbb{G}\) be the set of all legal game states.
Let \(\mathbb{G}_{done}\) be the set of all game states for which the game is done. At every gamestate one player is at turn. This means that that player will take an action next. Let \(\phi(s)~:~\mathbb{G}\to\{1,-1\}\) be the function mapping states to players. In the tick-tack-toe example from earlyer we could say that:
\begin{align}
\phi(s) = \left\{\begin{matrix}
1 & s \in g\cap X\\
-1 & s \in g\cap O
\end{matrix}\right.
\end{align}
More generally \(\phi(s) = 1\) if the first player (the player that starts the game) is at turn and \(\phi(s) = -1\) if the other player is a turn.

\subsubsection{Game Board}
Board games consist of placing stones of different types on a board with a certain amount of fields. Many games, like Go, Chess and Connect4, arrange their fields in a rectangular pattern. These games have two distinct stones. We can represent these game boards as stack of binary layers. Every layer is associated with one kind of stone. Each layer contains a one, where the board has a stone of the appropriate type and zeros everywhere else. For instance, the following tic tac toe game board can be represented by the following binary plane stack.\[
\ticTacToe{}{X}{O}{O}{X}{}{O}{}{X}\to
\left[ 
\begin{bmatrix}
0 & 1 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1 \\
\end{bmatrix}
\begin{bmatrix}
0 & 0 & 1 \\
1 & 0 & 0 \\
1 & 0 & 0 \\
\end{bmatrix}
\right]
\] 
Internally, the game board is represented by a flat vector. The conversion from a game state \(s  \in \mathbb G\) to vector is defined as \(vec(T_s(s))\). Where \(T_s(s):\mathbb G \to \mathbb R^{\cdots}\) is the board's 3-dimensional board tensor. The \(vec\) function is defined in \sectionref{sec:ref:vectorization:tensor}.
\begin{equation}
vec
\left(\left[ 
\begin{bmatrix}
a_{111} & \cdots & a_{11o} \\
\vdots & \ddots & \vdots \\
a_{1n1} & \cdots & a_{1no} \\
\end{bmatrix}
\cdots
\begin{bmatrix}
a_{m11} & \cdots &  a_{m1o} \\
\vdots   & \ddots & \vdots   \\
a_{mn1} & \cdots & a_{mno}  \\
\end{bmatrix}
\right]\right) = \left(\begin{matrix}
a_{111}      \\
\vdots        \\
a_{11o}      \\
\vdots        \\
a_{1n1}      \\
\vdots        \\
a_{1no}      \\
\vdots        \\
a_{m11}     \\
\vdots        \\
a_{m1o}     \\
\vdots        \\
 a_{mn1}    \\
\vdots        \\
 a_{mno}    \\
\end{matrix}\right)
\end{equation}
This operation for the tic tac toe board from before would look like this:
\begin{equation}
vec\left(T_s\left(\ticTacToe{}{X}{O}{O}{X}{}{O}{}{X}\right)\right) = 
[0\,1\,0\,0\,1\,0\,0\,0\,1\,0 \,0 \,1\,1\,0\,0\,1\,0\,0]^T
\end{equation}
\subsubsection{Actions}
Actions are numbers used to identify changes to the game. Every game has a set of all possible actions \(\mathbb{A}_{possible} \subset \mathbb{N}_0\). In connect4, the set of all possible actions for the current player is \(\mathbb{A}_{possible} = [0, 41]\). There is no need to have actions for the player, that is not at turn as these will never be taken. Every number is associated with a position on the game board. The mapping \(a\) to game fields is the following:
\begin{center}
\begin{tabular}{| c | c | c | c | c | c | c |}
 \hline
0 & 1 & 2 & 3 & 4 & 5 & 6  \\\hline
7 & 8 & 9 & 10 & 11 & 12 & 13\\\hline
14 & 15 & 16 & 17 & 18 & 19 & 20 \\\hline
21 & 22 & 23& 24 & 25 & 26 & 27 \\\hline
28 & 29 & 30 & 31 & 32 & 33 & 34 \\\hline
35 & 36 & 37 & 38 & 39 & 40 & 41 \\\hline
\end{tabular}
\end{center}
Let \(\mathbb{A}(s)\) be the set of all legal actions for a given state  \(s \in \mathbb{G}\). For all states \(s_{done} \in \mathbb{G}_{done}\) the set of all legal actions \(\mathbb{A}(s_{done})\) is the empty set. The function \(\mathcal{A}~:~\mathbb{G}\times\mathbb{A}\to\mathbb{G}\) is used to get from one game state to another by taking an action. Where \(\mathbb A\) is the set of all legal actions the chosen game state. If we were to map action to position for tick tack toe as follows and that the game board is the following:
\begin{center}\begin{tabular} { c c }
\begin{tabular}{| c | c | c |}
 \hline
 0 & 1 & 2 \\\hline
 3 & 4 & 5 \\\hline
 6 & 7 & 8 \\\hline
\end{tabular} & \tickTackToe{X}{O}{}{}{}{}{}{}{State \(s\)}
\end{tabular}\end{center}
In this example player \(X\) is allowed to place a stone in anny empy field \(\mathbb A(s) = \left\{ 2, 3, 4, 5, 6, 7, 8 \right\}\). Therefor \(\mathcal A(a, s)\) is valid if \(a \in \mathbb A(s)\) and otherwise invalid.

\subsection{MCTS}
A Monte Carlo tree search (MCTS) is a tree search algorithm that can be used to find sequences of actions leading to a desirable outcome. This is done by procedurally generating a directed graph of possible successor states to the current state or root state. In Alpha Zero it is used to improve the neural networks prediction.

A MCTS changes during simulation. MCTS simulation consists of three phases \nameref{sec:Methods:MCTS:Leaf_selection}, \nameref{sec:Methods:MCTS:Node Evaluation and Expansion} and \nameref{sec:Methods:MCTS:Backfill}.

This graph consists of nodes and edges. The edges represent connections betwean nodes while the nodes represent game states. Let \(\mathbb{M}_{possible} = \{\mathcal{N}(s)~|~ s \in \mathbb{G}\}\) be the set of all possible nodes , where \(\mathcal{N}:\mathbb{G}\to\mathbb{M}_{possible}\) is a bijective function that maps a game state to a node. \(\mathcal{S}~:~\mathbb{M}_{possible}\to\mathbb{G} = \mathcal{N}^{-1}\) is the inverse of \(\mathcal{N}\). For notational simplicity, the set of all allowed actions for any node \(n\in\mathbb{M}_{possible}\) is \(\mathbb{A}(n) = \mathbb{A}(\mathcal{S}(n))\). The ammount of nodes and structure of the MCTS is changed by the algorythem itself during simulation. Therefor Let \(l\) be the index of the current simulation step. Let \(\mathbb L = \mathbb [0, S]\) be the set of all \(l\), where \(S\) is a constant defined in the algorythems configuration file (\sectionref{code:config}). At the first step of the simulation, the Tree contains only the root node \(n_0\). The root node is the node is the starting point of the simulation. Let \(\mathbb{M}_l \subseteq \mathbb{M}_{possible}\) be the set of all nodes in the tree at step \(l\in\mathbb L\).

 Every node \(n_l \in \mathbb{M}_l\) has a set of edges \(\mathbb{E}_l(n_l)\) at step \(l\) that connect it to other nodes. Let \(\mathbb{E}_l\) be the set of all edges in the current graph. \(\mathcal{E}_l:~\mathbb{M}_l\times \mathbb{A}_{possible} \to \mathbb{E}_l\) is a bijective function used to map nodes and actions to edges. Furthermore for \(\mathcal{E}_l(n_l, a)\) to be valid \(a\) must be an element of \(\mathbb{A}(n_l)\).  The function \(\mathcal{N}_{to_l}~:~\mathbb{E}_{l}\to\mathbb{M}_l\) maps an edge to the node it is pointing to while \(\mathcal{N}_{from_l}~:~\mathbb{E}_{l}\to\mathbb{M}_l\) is used to find the node an edge is pointing from.
Furthermore it is usefull to distingush expanded nodes from leaf nodes. Leaf nodes are nodes that don't have edges leading out of them. Let \(\mathbb{M}_{leaf_l} \subseteq \mathbb{M}_l\) be the set of leafnodes. For every node \(n_l\in \mathbb{M}_{leaf_l}\), the following is true by definition \(\mathbb{E}_l(n_l) = \emptyset\). Expanded nodes are have edges leading out of them. Let \(\mathbb{M}_{expanded_l} \subseteq \mathbb{M}_l\) be the set of expanded nodes. For every node \(n_l\in \mathbb{M}_{leaf_l}\), the following is true by definition \(\mathbb{E}_l(n_l) \neq \emptyset\).

\begin{figure}[H]
  \centering
  \includesvg[height=.25\textheight]{images/mctsExample}
  \captionsetup{width=.9\linewidth}
  \caption{In the following small MCTS tree represents a possible graph at step \(l\), where \(n_?\) are nodes, \(e_?\) are edges connecting the nodes and \(A_?\) are actions associated with the edge they are next to. The questionmark represents the indices. The set of all nodes \(\mathbb M_l = \{n_1, n_2, \dots, n_8\}\) and \(\mathbb E_l = \{e_1, e_2, \dots, e_5\}\). We can also see that \(\mathbb E_l(n_1) = \{e_1, e_2, e_3\}\), \(\mathbb E_l(n_2) = \{e_4, e_5\}\), \(\mathbb E_l(n_3) = \emptyset\) and so forth. Because \(e_1\) represents action \(A_1\) taken at node \(n_1\), \(\mathcal E_l(n_1, A_1) = e_1\). By the same logic \(\mathcal E_l(n_2, A_2) = e_2\) and so forth. Becuase edge \(e_1\) connects node \(n_1\) to \(n_3\), \(\mathcal N_{from_l}(e_1) = n_1\) and \(\mathcal N_{to_l}(e_1) = n_3\). The same logic applies to all other nodes. The set of all expanded nodes \(\mathbb M_{expanded_l} = \{n_1, n_2, n_4\}\) because they have at least one edge leading away from them. On the other hand, the set of all leaf nodes \(\mathbb M_{leaf_l}  = \{n_3, n_5, n_6, n_7, n_8\}\). Finally \(\mathbb M = \mathbb M_{expanded} \cup \mathbb M_{leaf}\)
(see \sectionref{mctsExampleFullData} for all data)}
	\label{fig:mcts:example}
\end{figure}

\subsubsection{Evaluation Basis}
The MCTS's goal is to find a good estimations of the reward for a certain action at a certain state. This reward estimation is \(Q_l~:~\mathbb{E}_{l}\to \mathbb{R}\). To define \(Q_l\), the functions \(W_l~:~\mathbb{E}_{l}\to\mathbb{R}\) and \(N_l~:~\mathbb{E}_{l}\to\mathbb{N}_0\) are required. \(N_l(e_l)\) is the amount of times an edge \(e_l\in\mathbb{E}_{l}\) has been traversed. This means how many times \(\sigma\) (\equationref{eq:sigma}) has chosen to follow the edge \(e_l\) to a new node. \(W_l(e_l)\) is the sum of the reward computations from all \(N(e_l)\) times the edge has been evaluated. With these two functions \(Q_l\) is defined as:
\begin{equation}
Q_l(e_l) = 
\begin{cases}
0 & N_l(e_l) = 0\\
\frac{W(e_l)}{N(e_l)} &
\end{cases}
\end{equation}
The fourth and last of these functions is \(P_l~:~\mathbb{E}_{l}\to\mathbb{R}\). \(P_l\) is the policy function, it's the neural network's preliminary estimation of how valuable the action is. To see what \(P_l(e_l)\) is trained to aproximate see \equationref{eq:NN:policy_computation}.
This function is used to guide the search to more promising edges.

\subsubsection{Leaf Selection} \label{sec:Methods:MCTS:Leaf_selection}
MCTS's evaluation starts by simulating future moves within the tree. This is done by selecting an edge and then following that edge to a new node. From there, the next edge and node are selected. This is repeated until a leaf node is reached. To select an edge and thus a node from the current node \(n\in \mathbb{M}_l\) the function \(\sigma_l\) is used. To define \(\sigma_l\) we must first define the edge evaluation function \(v_l~:~\mathbb{E}_{l}\to\mathbb{R}\). \(v_l\) is defined as follows:
\begin{equation}
v_l(e) = Q_l(e) + c_{puct}P_l(e)\cdot\frac{\sqrt{\Sigma_{b\in\mathbb{E}(\mathcal{N}_{from_l}(e))}N_l(b)}}{1+N_l(e)}
\end{equation}
Where \(c_{puct}\in \mathbb{R}^+\) is the exploration constant used to define how important exploration is. The smaller \(c_{puct}\) is, more important \(Q\) and less important exploration and \(P\). \(\sigma_l\), for a given node \(n\in\mathbb{M}_l\), is then defined as:
\begin{equation}\label{eq:sigma}
\sigma_l(n) = \mathcal{N}_{to_l}(\text{argmax}_l(\mathbb{E}_l(n)))
\end{equation}
\(\text{argmax}_l\) returns the edge \(e \in\mathbb E_l\) with the largest \(v_l(e)\).
\(\sigma_l\) is run, until its output is a leaf-node \(n_{L_l}\in\mathbb{M}_{leaf_l}\).
\subsubsection{Node Evaluation and Expansion}
\label{sec:Methods:MCTS:Node Evaluation and Expansion}
When a leaf node \(n_{L_l}\) is reached, there are two possible cases. Ether \(\mathcal S(n_{L_l})\) is an end game state or not.
\paragraph{end game state \(\mathcal S(n_{L_l}) \in \mathbb G_{done}\)}
In this case the tree is not changed but backfill is still performed. The value \(v_l \mathbb R_{-1,1}\) \quckeq{set:Rab} of \(n_{L_l}\) is defined as if the curren player \(\phi(\mathcal S(n_{L_l}))\):
\begin{equation}
v_l = \left\{\begin{matrix}
1 &  \text{lost}\\
0 & \text{tied}\\
-1 & \text{won}
\end{matrix}\right.
\end{equation}
This is done because the players alternate taking turns.
\paragraph{not end game state \(\mathcal S(n_{L_l}) \not\in \mathbb G_{done}\)}
In this case the nodes state is passed to the neural network (\sectionref{NN}). The neural networks prediction function outputs a policy vector \(\pi_l \in \mathbb R_{0,1}^{|\mathbb A|}\) \quckeq{set:Rab} and a scalar \(v_l \in \mathbb R_{-1,1}\) \quckeq{set:Rab}. The scalar \(v_l\) is the neural networks estimation of the expected reward at \(n_{L_l}\). \(\pi_l\) is the estimation of the advantageousness of every action in \(\mathbb A\). After evaluation, the leaf node is expanded.
Let \(\mathbb E_{new}\) be the next set of all edges leading away from \(n_{L_l}\). This set contains one edge for every action in \(\mathbb A(n_{L_l})\).
Let \(\mathbb M_{new}\) be the set of nodes, that can be reached in one game step from \(n_{L_l}\).
The various Tree sets and functions are then updated as follows for step \(l+1\).
\begin{align}
\mathbb M_{l+1}   =& \mathbb M_l \cup \mathbb M_{new} \\
\mathbb E_{l_+1}  =& \mathbb E_l  \cup \mathbb E_{new} \\
%
\mathbb E_{l+1}(n) &= 
\left\{
\begin{matrix}
\mathbb E_{new} & n = N_{L_l}\\
\mathbb E_l(n)
\end{matrix}
\right.\\
%
\mathcal E_{l+1}(n, a) &=
\left\{
\begin{matrix}
\mathcal E_l (n, a) & n\neq n_{L_l}\\
e &
\end{matrix}
\right.
\end{align}
Where \(e \in\mathbb E_{new}\) is the edge associated with action \(a\) at node \(n_{L_l}\).
\begin{align}
\mathcal N_{to_{l+1}}(e) = 
\left\{
\begin{matrix}
\mathcal N_{to_l} (e) & e \in \mathbb E_l\\
n &
\end{matrix}
\right.
\end{align}
Where \(n\in\mathbb M_{new}\) is the node, that edge \(e\) is pointing to.
\begin{align}
\mathcal N_{from_{l+1}}(e) &= 
\left\{
\begin{matrix}
\mathcal N_{from_l} (e) & e \in\mathbb E_l\\
n _{L_l}&
\end{matrix}
\right.\\
\mathbb M_{expanded_{l+1}} &= \left\{n\in\mathbb M_{l+1}~:~\mathbb E_{l+1}(n) \neq \emptyset\right\}\\
\mathbb M_{leaf_{l+1}} &= \left\{n\in\mathbb M_{l+1}~:~\mathbb E_{l+1}(n) = \emptyset\right\}\\
P_{l+1}(\mathcal E_{l+1}(n, a)) &= 
\left\{
\begin{matrix}
P_l (\mathcal E_{l+1}(n, a)) & e \in\mathbb E_l\\
\pi_{l_a}&
\end{matrix}
\right.
\end{align}




\subsubsection{Backfill}
\label{sec:Methods:MCTS:Backfill}
Let \(\mathbb E_{traversed}_l\) be the set of all edges traversed during leaf selection.
The value \(v_l\) is used to update the reward prediction for all nodes \(n_{[0, L]_l}\) traversed during the edge selection. 
\begin{align}
N(n_t) + 1 &\Rightarrow N(n_t)\\
W(n_t) + v \cdot \rho(n_t) \cdot \rho(n_L) &\Rightarrow W(n_t)
\end{align}
Wher \(\Rightarrow\) is defined in \sectionref{sec:assignmentOperator}.

%-------------------------------------------------------------------------
\subsection{Neural Network}
Search algorithms like MCTS are able to find advantageous action sequences. In game engines, the search algorithm is improved by using evaluation functions. These functions are generally created using human master knowledge. In the Alpha Zero algorithm, this evaluation function is a biheaded deep convolutional neural network trained by information gathered from the MCTS. In order to understand the training process, one must first understand how the neural network functions.
\label{NN}
\subsubsection{Introduction to Neural Networks}
An artificial neural network or just neural network is a mathematical function inspired by biological brains. Although there are many types of neural networks, the only relevant one to this work is the feed forward network. These models consist of multiple linear computational layers separated by non-linear activation functions. Every layer takes the outputs of the previous layer, and applies a linear transformation to it~\cite{zhang2018artificial}. There are many different feed-forward neural network layers and activation functions to chose from when designing a neural network. To focus this explanation, only the relevant ones will be discussed along with the back-propagation algorithm.
\paragraph{Fully Connected Layer}
A fully connected layer is the most basic layer. It applies a simple matrix multiplication. The layer takes a \(1 \times n\) dimentional matrix \(x \in \mathbb{R}^{1\times n}\) as an input and multiplies it by a weight matrix \(w \in\mathbb{R}^{n\times m}\). This operation outputs a \(1\times m\) dimensional matrix to which a bias \(b \in \mathbb{R}^{1 \times m}\) is added to form the output matrix \(v \in \mathbb{R}^{1 \times m}\) containing the output values of the layer. \(v\) is then fed to the next layer. The addition of the bias vector \(b\) is optional. In some situations it is worth dropping the bias in favour of computational speed. The fully connected layer forward propagation function shall be defined as \(\delta_{wb}~:~\mathbb{R}^n\to\mathbb{R}^m\)
\begin{equation} \label{eq:NN:fully_connected_layer_forward}
\delta_{wb}(x) = w \cdot x + b
\end{equation}
\paragraph{Convolutional Layer}
Convolutional layers are commonly used for image processing. They perform the same operations over the entire image searching for certain patterns. In order to achieve this, a set of kernels \(\mathbb{K}\), of size \(m\times n\), are defined for the layer. Kernels are similar to fully connected layers. They consist of a weight tensor \(w\in\mathbb{R}^{m \times n \times l}\) and an optional bias scalar \(b\in\mathbb{R}\). For every kernel \(k\in \mathbb{K}\), the kernel's forward operation \(\xi_k~:~\mathbb{R}^{m \times n \times l}\to\mathbb{R}\) is defined as:
\begin{equation}\label{eq:NN:kernelOpperation}
\xi_k(i) = \left<w_k, i\right>_I + b
\end{equation}
where \(<>_I\) is the Tensor inner product defined in \equationref{eq:defs:Inner_product_3d}.
The convolutional operation \(\Lambda~:~\mathbb{R}^{i\times j \times l}\to\mathbb{R}^{i-m+1 \times j-n+1 \times |\mathbb{K}|}\) is an element wise opperation. Given that  \(I \in \mathbb{R}^{i\times j\times l}\) is the layer input, every element of \(\Lambda(I)_{abc}\) with \(a \in [1, i-m+1]\), \(b \in [1, j-n+1]\) and \(c \in [1, |\mathbb{K}|]\) is defined as:
\begin{equation}\label{eq:NN:convolutional_opperation}
\Lambda(I)_{abc} = \xi_{k_c}(\left<I\right>_{S_{m\times n,\,ab}})
\end{equation}
The submatrix indexing operation \(\left<\right>_s\) is defined in \sectionref{sec:Ref:submatrix}.
For example given the following input tensor \(I \in\mathbb{R}^{4\times4\times1}\): % matrix slice
\[
I = \left[
\begin{matrix}
[3] & [0] & [1] & [5] \\
[2] & [6] & [2] & [4] \\
[2] & [4] & [1] & [0] \\
[3] & [0] & [1] & [5] \\
\end{matrix}
\right]
\]
and the following kernel weight matrix \(w_k \in \mathbb{R}^{3 \times 3 \times 1}\) along with the scalar \(b\in \mathbb{R}\),
\begin{align*}
w_k &= \left[
\begin{matrix}
[-1] & [0] & [1] \\ 
[-2] & [0] & [2] \\
[-1] & [0] & [1]
\end{matrix}
\right]\\
b &=7\\
\end{align*}
there are four possible locations in which \(w_k\) can be placed within \(I\). As there is only one kernel, the length of the set of all kernels \(|\mathbb{K}| = 1\). This also means that \(\Lambda(I) \in R^{2 \times 2 \times 1}\). To calculate \(\Lambda(I)_{111}\), we compute the kernel operation \(\xi_k(I[[1,3],[1,3],\{1\}])\)
\begin{equation}
\label{eq:conv:eg}
\Lambda_{111}
\left(\begin{bmatrix}
[3] & [0] & [1] & [5] \\
[2] & [6] & [2] & [4] \\
[2] & [4] & [1] & [0] \\
[3] & [0] & [1] & [5] \\
\end{bmatrix}\right)\\
 = \begin{bmatrix}
[3] & [0] & [1] \\
[2] & [6] & [2] \\
[2] & [4] & [1] \\
\end{bmatrix} \circ 
\begin{bmatrix}
[-1] & [0] & [1] \\
[-2] & [0] & [2] \\
[-1] & [0] & [1]
\end{bmatrix} + 7
\end{equation}
\begin{align*}
&= -1 \cdot 3 + 0 \cdot 0 + 1 \cdot 1 - 2 \cdot 2 + 0 \cdot 6 + 2 \cdot 2 -1 \cdot 2 + 0 \cdot 4 + 1 \cdot 1 + 7\\
&= 4\\
\end{align*}
The same is done for \(\Lambda(I)_{121}\), \(\Lambda(I)_{211}\) and \(\Lambda(I)_{221}\). This leads to a \(\Lambda(I)\) of:
\[
\Lambda(I) = \begin{bmatrix}
[4] & [3] \\
[4] & [2] \\
\end{bmatrix}
\]
\paragraph{Activation Function}
All neural network layers are linear functions. Thus, given two layer evaluation functions \(f_1(x) = ax + b\) and \(f_2(x) = cx + d\), the chained function \(f(x) = f_1(f_2(x))\) is also linear because:
\begin{equation}
\label{eq:proofOfLinearity}
f(x) = f_1(f_2(x)) = a(cx + d) + b = acx + ad + b = ex + g
\end{equation}
where \(a\),  \(b\), \(c\), \(d\), \(e\) and \(g \in \mathbb R\). In order to represent non linear functions, a non linear activation function \(f_a\) is added between two neural network layers. Thus, the chained function becomes \(c(x) = f_1(f_a(f_2(x)))\). In this neural network, three different activation functions are used: \(tanh\), \(softmax\), and \(LeakyReLU\). These functions are defined as follows:
\subparagraph*{tanh:}\(\mathbb{R}\to \mathbb{R}\)

\begin{center}
\includesvg[width=0.9\columnwidth]{images/tanh}
\captionof{figure}{tanh function in blue and the tanh's derivative is in orange}
\end{center}

\begin{equation} \label{eq:NN:tanh}
tanh(x) = \frac{e^x-e^{-x}}{e^x+e^{-x}}
\end{equation}\begin{equation} \label{eq:NN:tanh_derivative}
\frac{d}{dx}tanh(x) = sech(x)^2
\end{equation}
see \sectionref{sec:proof:tanhd/dx} for proof.
\subparagraph*{softmax:} \(\mathbb{R}^n\to \mathbb{R}^n\)\\
\indent For a given input vector \(v \in \mathbb{R}^n\). The output vector \(o \in \mathbb{R}^n\) at every position \(i \in [1, n]\) is:
\begin{equation} \label{eq:NN:softmax}
o_i = softmax(v)_i = \frac{e^{v_i}}{\sum_{j \in v} e^j}
\end{equation}
\begin{figure}[H]
\centering
\begin{subfigure}{.4\textwidth}
  \centering
  \includesvg[height=.25\textheight]{images/softmaxx}
  \caption{\(i=1\)}
  \label{fig:sub1}
\end{subfigure}%
\begin{subfigure}{.4\textwidth}
  \centering
  \includesvg[height=.25\textheight]{images/softmaxy}
  \caption{\(i=2\)}
  \label{fig:sub2}
\end{subfigure}
\begin{subfigure}{.1\textwidth}
  \centering
  \includesvg[height=.25\textheight]{images/softmaxscale}
  \caption*{scale}
\end{subfigure}
\caption{Graph of the softmax function from \(\mathbb R^2 \to \mathbb R^2\). \(i\) is the index of the output dimension. Therefore, \(i=1\) referes to the output's first dimension and \(i=2\) referes to it's second dimension.}
\label{fig:test}
\end{figure}

Because the function's in- and outputs are n dimensional vectors, the derivative is an \(n\times n\) dimensional matrix. When taking its derivative, \(\frac{d}{dv_j}softmax(v)_i\), there are two possible cases. 
\\\indent case \(j = i\):
\begin{equation}
\frac{d}{dv_j}\left[\frac{e^{v_i}}{\sum_{b\in v} e^{b}}\right] = softmax(v)_i\cdot (1 - softmax(v)_j)\\
\end{equation}
\\\indent case \(j \neq i\):
\begin{equation}
\frac{d}{dv_j}\left[\frac{e^{v_i}}{\sum_{b\in v} e^{b}}\right] = -softmax(v)_i\cdot softmax(v)_j \\
\end{equation}
Therefore, the derivative of the softmax function is:
\begin{equation}\label{eq:NN:softmax_derivative}
softmax'(v)_{ij} = \left\{
\begin{array}{ll}
 softmax(v)_i\cdot (1-softmax(v)_j) & i = j\\
 - softmax(v)_i \cdot softmax(v)_j     & i \neq j
\end{array}
\right.
\end{equation}
\subparagraph*{LeakyReLU:}\(\mathbb{R} \to \mathbb{R}\)
\begin{center}
\includesvg[width=0.9\columnwidth]{images/LeakyReLU}
\captionof{figure}{LeakyReLU with \(c = 0.3\)}
\end{center}

\begin{equation} \label{eq:NN:ReLU}
\text{LeakyReLU}(x) = \left\{
\begin{array}{ll}
x & x \ge 0 \\
x \cdot c & x < 0 \\
\end{array}
\right.
\end{equation}
\[
\frac{d}{dx} \text{LeakyReLU}(x) = \left\{
\begin{array}{ll}
\frac{d}{dx}x & x \ge 0 \\
\frac{d}{dx}c\cdot x & x < 0 \\
\end{array}
\right.
\]
\begin{equation} \label{eq:NN:Relu_derivative}
\text{LeakyReLU}'(x) = \left\{
\begin{array}{ll}
1 & x > 0 \\
c & x < 0 \\
\end{array}
\right.
\end{equation}
where \(c\) is a constant describing the slope of the function for negative input values. The derivative of the LeakyReLU function is undefined for \(x=0\). However as we will be performing gradient descent on these functions the derivative must befined for all \(x \in \mathbb R\). A possible definition that accomplisches the objective is:
\begin{equation}
g(x) = 
\begin{cases}
1 & x \ge 0\\
c & x < 0\\
\end{cases}
\end{equation}
\paragraph{Training} 
Neural network training can be mathematically expressed as minimizing a loss function  \(\ell\) describing how inaccurate the network is. In our case, \(\ell\) takes the neural network's predicted value vector \(Y_{pred}\) and the correct value vector \(Y_{true}\). \(Y_{true}\) must be known before the computation begins. In AlphaZero, \(Y_{true}\) is generated by the MCTS. As with the activation, function there are many different possible loss functions. In this implementation, the mean-square-error(\(mse\)) loss function is used. \(mse:~\mathbb R^n\times\mathbb R^n \to \mathbb R\) is defined as:
\begin{equation} \label{eq:NN:loss_mse}
\ell = \frac{|Y_{pred} - Y_{true}|^2}{n}
\end{equation}
The network then performs gradient descent to find parameters that minimize \(\ell\). To make this introduction easier, I will use a fully connected neural network. For every layer in the network, starting with the last one, it must be determined in which direction and by how much the output values \(Y_{pred_i} \in Y_{pred}\) should be ``moved'' to minimize \(\ell\). This change is described by \(\Delta Y_j\), where \(j\) is the index of the last layer. Then, the change in the inputs to the activation function \(f_a\) must be computed using the saved activation function inputs \(A\). \(\Delta A\) will describe the change to \(A\).
\begin{equation}\label{eq:NN:deltaA}
\Delta A = f_a'(A) \circ \Delta Y_j
\end{equation}
The hadamard product \(\circ\) is 
defined in \sectionref{sec:hadamerd_product}.\\
Next comes the update to the weight matrix \(w\). Let \(\Delta w\) describe the change to \(w\) and let \(X\) be the input vector of the layer. \(\Delta w\) is than defined as:
\begin{equation}\label{eq:NN:deltaW}
\Delta w = \Delta A \cdot X^T
\end{equation}
The layer's bias vector is updated in the direction of \(\Delta A\):
\begin{equation}\label{eq:NN:deltaB}
\Delta b \sim \Delta A
\end{equation}
Lastly, the change to the output of the previous layer \(\Delta Y_{j-1}\) is computed.
\begin{equation} \label{eq:NN:deltaA_lastLayer}
\Delta Y_{j-1} = \Delta A \cdot w^T
\end{equation}
This process is repeated until the foremost layer of the neural network is reached. This layer has the index \(j=0\).

\subsubsection{Network used by AlphaZero}
The neural network in Alpha Zero is used to estimate the value \(v\) and policy \(p\) for any game state or node \(n\). \(v\) is the neural network's estimation of the state's expected reward. The policy \(p \in\mathbb{R}^{|\mathbb{A}|}\) of a game state \(n\) represents the advantageousness of every action \(a\in\mathbb{A}\), as estimated by the neural network.
\paragraph{Neural Network input}\label{sec:neuralNetworkInput}
The neural network input is a game state or node \(n\) represented by two \mbox{7 x 6} binary images stacked on top of each other. One image \(X\) represents the stones belonging to the current player. While the second image \(Y\) represents the stones belonging to the other player. In both images, the pixel values are one where a stone belonging to the player they represent is located and zero if the field is empty or a stone belonging to the other player is located there. \(X\) and \(Y\) are then stacked on top of each other in the third dimension to form the input tensor \(i_n = [X, Y] \in \mathbb{R}^{7 \times 6 \times 2}\).
Consider the following Connect4 board \imgRef{fig:connect4ImageOne}.

\begin{center}
\includegraphics[width=0.5\textwidth]{connectFourExample}
\captionof{figure}{}
\label{fig:connect4ImageOne}
\end{center}

If red is the current player then:
\[
X = \left[
\begin{matrix}
\mathColor{black}{0} & \mathColor{black}{0} & \mathColor{black}{0} & \mathColor{black}{0} & \mathColor{black}{0} & \mathColor{black}{0} & \mathColor{black}{0}\\
\mathColor{black}{0} & \mathColor{black}{0} & \mathColor{black}{0} & \mathColor{red}{1}     & \mathColor{black}{0} & \mathColor{black}{0} & \mathColor{black}{0}\\
\mathColor{black}{0} & \mathColor{black}{0} & \mathColor{\gold}{0} & \mathColor{\gold}{0} & \mathColor{black}{0} & \mathColor{black}{0} & \mathColor{black}{0}\\
\mathColor{black}{0} & \mathColor{black}{0} & \mathColor{red}{1}     & \mathColor{\gold}{0} & \mathColor{black}{0} & \mathColor{black}{0} & \mathColor{black}{0}\\
\mathColor{black}{0} & \mathColor{black}{0} & \mathColor{red}{1}     & \mathColor{red}{1}     & \mathColor{\gold}{0} & \mathColor{black}{0} & \mathColor{black}{0}\\
\mathColor{black}{0} & \mathColor{black}{0} & \mathColor{red}{1}     & \mathColor{\gold}{0} & \mathColor{\gold}{0} & \mathColor{red}{1}     & \mathColor{black}{0}\\
\end{matrix}
\right]
\]\[
Y= \left[
\begin{matrix}
\mathColor{black}{0} & \mathColor{black}{0} & \mathColor{black}{0} & \mathColor{black}{0} & \mathColor{black}{0} & \mathColor{black}{0} & \mathColor{black}{0}\\
\mathColor{black}{0} & \mathColor{black}{0} & \mathColor{black}{0} & \mathColor{red}{0}     & \mathColor{black}{0} & \mathColor{black}{0} & \mathColor{black}{0}\\
\mathColor{black}{0} & \mathColor{black}{0} & \mathColor{\gold}{1} & \mathColor{\gold}{1} & \mathColor{black}{0} & \mathColor{black}{0} & \mathColor{black}{0}\\
\mathColor{black}{0} & \mathColor{black}{0} & \mathColor{red}{0}     & \mathColor{\gold}{1} & \mathColor{black}{0} & \mathColor{black}{0} & \mathColor{black}{0}\\
\mathColor{black}{0} & \mathColor{black}{0} & \mathColor{red}{0}     & \mathColor{red}{0}     & \mathColor{\gold}{1} & \mathColor{black}{0} & \mathColor{black}{0}\\
\mathColor{black}{0} & \mathColor{black}{0} & \mathColor{red}{0}     & \mathColor{\gold}{1} & \mathColor{\gold}{1} & \mathColor{red}{0}     & \mathColor{black}{0}\\
\end{matrix}
\right]
\] For clarification, the numbers are coloured in the same colour as the stones at that position. After stacking \(X\) and \(Y\), \(i_n\) is:
\[
i_n = \left[
\begin{matrix}
\mathColor{black}{[0,0]} & \mathColor{black}{[0,0]} & \mathColor{black}{[0,0]} & \mathColor{black}{[0,0]} & \mathColor{black}{[0,0]} & \mathColor{black}{[0,0]} & \mathColor{black}{[0,0]}\\
\mathColor{black}{[0,0]} & \mathColor{black}{[0,0]} & \mathColor{black}{[0,0]} & \mathColor{red}{[1,0]}     & \mathColor{black}{[0,0]} & \mathColor{black}{[0,0]} & \mathColor{black}{[0,0]}\\
\mathColor{black}{[0,0]} & \mathColor{black}{[0,0]} & \mathColor{\gold}{[0,1]} & \mathColor{\gold}{[0,1]} & \mathColor{black}{[0,0]} & \mathColor{black}{[0,0]} & \mathColor{black}{[0,0]}\\
\mathColor{black}{[0,0]} & \mathColor{black}{[0,0]} & \mathColor{red}{[1,0]}     & \mathColor{\gold}{[0,1]} & \mathColor{black}{[0,0]} & \mathColor{black}{[0,0]} & \mathColor{black}{[0,0]}\\
\mathColor{black}{[0,0]} & \mathColor{black}{[0,0]} & \mathColor{red}{[1,0]}     & \mathColor{red}{[1,0]}     & \mathColor{\gold}{[0,1]} & \mathColor{black}{[0,0]} & \mathColor{black}{[0,0]}\\
\mathColor{black}{[0,0]} & \mathColor{black}{[0,0]} & \mathColor{red}{[1,0]}     & \mathColor{\gold}{[0,1]} & \mathColor{\gold}{[0,1]} & \mathColor{red}{[1,0]}     & \mathColor{black}{[0,0]}\\
\end{matrix}
\right]
\]
\paragraph{Neural Network Architecture} 
\label{sec:NeruralNetworkArchitecture}
The neural network used by Alpha Zero consists of three main sub-modules, namely the residual tower, the value head and the policy head. The residual tower's purpose is to preprocess the data for the two heads. The value head determines the value \(v\) from the output of the residual tower. While the policy head computes the policy \(p\). The residual tower consists of a convolutional block followed by six residual blocks.\\
The convolutional block consists of the following: 
\begin{enumerate}
\item A convolutional layer consisting of 75 filters with a kernel size of 3 x 3
\item Batch normalization \cite{ioffe2015batch}
\item A non-linear rectifier (LeakyReLU).
\end{enumerate}
Every residual block consists of the following modules:
\begin{enumerate}
\item A convolutional layer consisting of 75 filters with a kernel size of 3 x 3
\item Batch normalization \cite{ioffe2015batch}
\item A non-linear rectifier (LeakyReLU)
\item A convolutional layer consisting of 75 filters with a kernel size of 3 x 3
\item Batch normalization \cite{ioffe2015batch}
\item Batch normalization outputs are added to the block's input.
\item A non-linear rectifier (LeakyReLU)
\end{enumerate}
Outputs are then passed to the value and policy head of the network for further evaluation.
The value head consists of the following modules:
\begin{enumerate}
\item A convolutional layer consisting of 10 filters with a kernel size of 1 x 1
\item A fully connected layer of size 210
\item A non-linear rectifier (LeakyReLU)
\item A fully connected layer of size 1
\item \(tanh\) activation function
\end{enumerate}
The policy head consists of the following modules:
\begin{enumerate}
\item A convolutional layer consisting of 2 filters with a kernel size of 1 x 1
\item A fully connected layer of size 1
\end{enumerate}
The output of the policy head \(p_{pre}\) is then masked with the allowed actions to form \(p_{masked}\) in such a way that \(p_{masked}\) is \(-1000\) for all non-allowed actions. Finally, \(p_{masked}\) is passed throught the softmax function to form \(\pi\):
\begin{equation}\label{eq:nn:policyDefinition}
\pi = \text{softmax}(p_{masked})
\end{equation}

\paragraph{Training}\label{sec:Methods:NN:A0:Training}
Training is performed in batches of \(256\) states. The value head is updated using mean square error. The policy head is updated using mean square error as well. However all non-legal actions are ignored. This avoids unnecessary updating of the neural network. The value, the neural network is trained to predict for a certain MCTS node \(n\), is equivalent to 1 if the player who took an action at node \(n\) did win, \(-1\) if that player did lose and \(0\) if the game ended in a tie. The policy \(p_{a_l}\) to train for, for a given legal action \(a_l \in \mathbb A(n)\) is:
\begin{equation} \label{eq:NN:policy_computation}
p_{a_l} = \frac{N(n, a_l)}{\sum_{a\in \mathbb A(n)} N(n, a)}
\end{equation}
For non legal actions \(a_n \in (\mathbb A_{possible} - \mathbb A(n))\), \(p_{a_n}\) is defined as:
\begin{equation} \label{eq:NN:policy_computation_nonlegal}
p_{a_n} = p_{pre_{a_n}}
\end{equation}Ë‡


\subsection{Data generation}
The data used to train the neural network is generated by letting the best agent play several games against itself, until enough data has been generated to allow for training. In every game, at every game state, the MCTS performs 50 simulations. Once the simulations are done the action is chosen.
\subsubsection{Action selection}\label{sec:training:actionSelection}
There are two methods for action selection for a given node \(n_t\): deterministic and probabilistic. The first will always return the action \(a = argmax(N(\mathcal{E}(n_t, a \in \mathbb A(n_t))))\) of the most traversed edge, while the second will return a random action where the probability of selecting an action \(a_i \in \mathbb A(n_t)\) is:
\begin{equation} \label{eq:ActionSelection:Probabilistic}
P(X=a_i, n_t) = \frac{N(\mathcal{E}(n_t, a_i))}{\sum_{j \in \mathbb A(n_t)} N(\mathcal{E}(n_t, j))}
\end{equation}
(\(\mathbb A(s)\) are the allowed actions for state \(s\).) Action selection during the training phase shall initially be probabilistic, and deterministic later on. The handover point shall be defined as the configurational constant 'probabilistic\_moves' \(\in \mathbb N^+\). During games outside the training loop, actions are always selected deterministically.
 \subsubsection{Memory}
\label{sec:memory}
The memory stores a certain amount of memory elements. A memory element consists of a gamestate \(g \in \mathbb G\), its action values \(v \in \mathbb R^{|\mathbb A|}\) and the true reward \(r \in \mathbb \{1, -1, 0\}\) = \(R(g, a)\) where \(a\) is the action taken during play at that game state. The memory stores memory elements in a long list. After an action has been selected, but before any updates to the game simulation are made, the current game state is passed to temporary memory along with its action values \(v\). Together they create a new memory element. This element's \(r\) is currently undefined. \(v\) is defined as: 
\begin{equation} \label{eq:Memory:ActionValuesDefinition}
v_a = 
\begin{cases}
P(X=a, \mathcal N(g)) & a \in \mathbb A(g)\\
p_{pre_a} &
\end{cases}
\end{equation}
\(\mathbb A(g)\) is the set of all legal actions. \(p_{pre}\) is defined in \sectionref{sec:NeruralNetworkArchitectur}, and is used for all non legal actions. \(P\) is defined in \equationref{eq:ActionSelection:Probabilistic}.

\paragraph{Memory update}
Once the game is over, the winning player is determined and the value \(r\) of every memory element in the temporary memory is updated. \(r\) is 1 if the player taking an action at that state won, -1 if he lost and 0 if the game ended in a draw. The updated states are then passed to memory.

\paragraph{Model Training}
Once the memory size exceeds 30'000 states, the self-playing stops and the neural network is trained as described in section:  \ref{sec:Methods:NN:A0:Training}.

\subsection{Model evaluation}\label{sec:modelEvaluation}
In order to train the neural network, the "best player" generates data used to train the current network.
After every time the current neural network has been updated, it plays 20 games against the best player. If it wins more than \(1.3\) times as often as the current best player, it is considered better. If this is the case, the neural network of the "current player" is saved to file and the old "best player" is replaced with the "current player" to become the new "best player". It is advantageous to force the network to win \(1.3\) times as often as that reduces the chance of the network just getting lucky. 
\section{Evaluation}
To give us an idea of how good a player is, it would be useful to express performance using a single number. This number should not only give us a ranking but also allow for predictions of the winner of a game between two players and thus give us a measure of the relative strength of the players. One such rating method is the so called elo-rating method. \cite{elo1978rating}
\subsection{Elo-rating} \label{sec:Evaluation:elo-rating}
The elo-rating system assigns every player \(p\) a number \(r_p \in \mathbb{R}\). In general, the larger \(r_p\) the better the player. More specifically, given two players \(a\) and \(b\) with elo-ratings \(r_a\) and \(r_b\), the expected chance \(E\) of \(a\) winning against \(b\) is \cite{silver2018general}:
\begin{equation} \label{eq:elo_pred}
E = \frac{1}{1 + e^{(r_b-r_a)/400}}
\end{equation}
\begin{center}
\includesvg[width=0.8\columnwidth]{images/elo}
\captionof{figure}{elo-rating win probability for \(r_b = 0\)}
\end{center}
This function describes a sigmoid curve. This makes sense, because if the players have major strength discrepancies \(E\) converges to \(1\) or \(0\). When \(a\) and \(b\) play a game against each other, their elo,ratings are updated as follows\cite{elo1978rating}:

\begin{equation} \label{eq:elo_update}
r_{n+1} = r_n + K(W - E)
\end{equation}
with:
\begin{description}
\item \(r_{n+1}\) the new rating for the player.
\item \(r_{n}\) the current rating of the player.
\item \(W = s_a\) which is defined by equation \ref{eq:elo_score} where \(a\) is the player to be updated.
\item \(E\) the expected chance of winning, see equation \ref{eq:elo_pred}.
\item \(K\) is a constant controlling the sensitivity of the update function.
\end{description}
However, to avoid slow convergence of elo-ratings, a more direct formula is used to approximate the rating of an agent \(a\). This is done by playing a predetermined amount of games against player \(b\) whose elo-rating \(r_b\) is known and unchanged throughout this process. First, \(a\) and \(b\) play a predetermined amount of games \(m\) and the score \(s_a\) of \(a\) is computed as \cite{elo1978rating}:
\begin{equation} \label{eq:elo_score}
s_a = \frac{1}{m}\sum\left\{
\begin{array}{ll}
1 &              a\textrm{ wins} \\
\frac{1}{2} & \textrm{tie}\\
0 &              a\textrm{ looses}\\
\end{array}
\right.
\end{equation}
Assuming that this is the probability of \(a\) winning against \(b\), \(a\)'s elo-rating can be computed by solving equation \ref{eq:elo_pred} to \(r_a\) \imgRef{fig:eloInv}:
\begin{equation} \label{eq:elo_back}
r_a = r_b - ln\left(\frac{1-s_a}{s_a}\right) \cdot 400
\end{equation}
\begin{center}
\includesvg[width=0.8\columnwidth]{images/elo_back}
\captionof{figure}{elo inverse function}
\label{fig:eloInv}
\end{center}
Since a ranking of all the agents already exists (see \sectionref{sec:modelEvaluation}), an agent's elo-rating can be computed by playing against an older version and then using equation \ref{eq:elo_back} to determine its elo-rating. 
\subsubsection{Relativity of the Elo-rating}
The only problem is that elo is a relative rating. The rating of any other agent depends on its performance against other agents and their elo-ratings. Therefore, one must give the system a base rating for at least one predefined agent. In this case, there are no previously known elo-rated agents , so I defined the untrained agent's elo-rating as 100. All other elo-ratings are relative to that.

\subsection{Elo results}
The rating \(r_i\) of any agent version \(i\) must in general be greater than the rating of the last version \(r_i > r_{i-1}\). Furthermore, the expected minimal increase in rating \(\Delta r_{min} = r_i - r_{i-1}\) is:
\begin{equation}
\Delta r_{min} = -ln\left(\frac{1-s_i}{s_i}\right)\cdot 400
\end{equation}
As a certain scoring threshold \(\theta = 1.3\) was used during training to minimize the effect of noise in the evaluation, a prediction of \(s_i\) can be made. Given that \(s_a\) and \(s_b\) are the scores of two players that play aginst each other, then by definition:
\begin{equation}
s_a + s_b = 1
\end{equation}
Due to the imposed scoring threshold \(\theta\) and the assumption that there are no ties:
\begin{equation}
s_a \geqslant s_b \cdot \theta
\end{equation}
(if \(s_a\) has a higher version number than \(s_b\))\\\\For \(\theta = 1.3\) this means that the expected average change in rating \(\Delta r\):
\begin{equation}
\Delta r \geqslant -ln\left(\frac{1}{\theta}\right)\cdot 400 = \Delta r_{min} \cong 105
\end{equation}
Collected data shows this to be true \imgRef{img:eloByVersion}. The same data shows that the average \(\Delta r \) is in fact roughly \(\FittedEloRaiting\), which would equate to a \(\theta\) of

\begin{equation}
\theta = \frac{1}{e^{\frac{-\Delta E}{400}}} \cong \FittedScoringThreshold
\end{equation}
\begin{center}
	\includesvg[width=0.9\textwidth]{eloRaitingByVersion}
	\captionsetup{width=.8\linewidth}
	\captionof{figure}{Elo-rating of agents based on their version along with the expected minimal rating \(\Delta r_{min}\) and the best fitted rating \(\Delta r\).}
	\label{img:eloByVersion}
\end{center}
\section{Servers and Clients}
In computer science, server-client-communications are a form of distributed application, that allows multiple machines to communicate and share data. In general, the server will wait for connections, while the client will initialize a connection with the server. To accomplish this, the server must listen to a certain port and the client must know the servers ip and port. In our case, the cummunications use the TCP and HTTPS protocols. Alpha Zero uses three distinct servers: an AI server, a data server and an Apache web server.
\subsection{The Web Server}
Alpha Zero's web server uses the Apache web server application. The web server is used to host static files such as the source code for the iOS client (see \sectionref{sec:iosClient}), a debug version of the same client, the domain name of the AI server and the domain name of the data server. All these files were located at \href{https://wandhoven.ddns.net/code/AlphaZero/}{https://wandhoven.ddns.net/code/AlphaZero/}
\subsection{The Data Server}
The data server stores all global information. This was just the elo-rating to begin with, but was later expanded to handle all data. This explains the somewhat strange communication protocol. Requests to the server begin by sending a 4 byte signed integer \(a\) identifying the general action the server must perform. The first action \(a=1\) will return the elo-rating of a certain agent.
This will require a further 4 bytes identifying the agent. The second action \(a=2\) will set an agents elo-rating. Two 4 byte integers are sent, the first identifying the agent and the second the elo-rating to set to. The third action \(a=-1\) will require a 4 byte signed integer \(e\) and return the agent's identifier with an elo-rating equal to \(r\) defined as:
\begin{equation}
r = \text{min}(\left\{x \in\mathbb E | x \geq e \right\})
\end{equation}
where \(\mathbb E\) is the set of the elo-ratings of all agents. The last action \(a=-2\) will access the custom data part of the server. This subsection will require an integer describing how many bytes the request consists of. The request is encoded using the python pickle library and consists of either a tuple containing a string, and a list of strings; or a tuple containing a string, a list of strings, and any other data type. In the first case, the system will return the value of the saved data associated with that request. In the second case, the value of the associated the variable will be set to whatever the third value is.\\\\
The first two variables of the tuple are a string \(f\) and a list of strings \(k\). \(f\) tells the server in which file the variable is stored. Therefore, the server will load the json file with the name \(f\). \(k\) is the list of keys used to index the dictionary \(f\). \\\\
For example, with \(f = \text{example.json}\), the server would decode the json file "example.json" shown in \listingref{listing:dataAssociation}. Assuming that \(k =\) ["address", "city"], the server would first search for key "address" in the outer most directory. At that key, there is another dictionary, which is then searched for the key "city".  At that key, there is a string ("New York"), which would  be returned to the client.

\begin{listing}[H]
\begin{minted}[frame=single,
               framesep=3mm,
               linenos=true,
               xleftmargin=21pt,
               tabsize=4]{js}
{
  "firstName": "John",
  "lastName": "Smith",
  "isAlive": true,
  "age": 27,
  "address": {
    "streetAddress": "21 2nd Street",
    "city": "New York",
    "state": "NY",
    "postalCode": "10021-3100"
  },
  "phoneNumbers": [
    {
      "type": "home",
      "number": "212 555-1234"
    },
    {
      "type": "office",
      "number": "646 555-4567"
    }
  ],
  "children": [],
  "spouse": null
}
\end{minted}
\caption{example.json \newline\indent from \href{https://en.wikipedia.org/wiki/JSON}{https://en.wikipedia.org/wiki/JSON}} 
\label{listing:dataAssociation}
\end{listing}
\subsection{The AI Server}
The AI server is used to evaluate a state and determine the best action using Alpha Zero. This is done by sending the server the state and waiting for it to send back the action.
\subsubsection{State Transmission}
To send a state to the server, the state's \(6 \times 7\) board is converted into an array of 85 boolean values\footnote{Stored as integers 0 and 1 in memory}. The first 42 booleans represent whether or not the starting player has a stone at that position. Positions are mapped from left to right and then top to bottom. The table below shows the order in which the positions will be added to the boolean array. The next 42 booleans are identical to the first but for the non-starting player. The last position tells the server which player is taking the next action.
\begin{center}
\begin{tabular}{| c | c | c | c | c | c | c |}
 \hline
0 & 1 & 2 & 3 & 4 & 5 & 6  \\\hline
7 & 8 & 9 & 10 & 11 & 12 & 13\\\hline
14 & 15 & 16 & 17 & 18 & 19 & 20 \\\hline
21 & 22 & 23& 24 & 25 & 26 & 27 \\\hline
28 & 29 & 30 & 31 & 32 & 33 & 34 \\\hline
35 & 36 & 37 & 38 & 39 & 40 & 41 \\\hline
\end{tabular}
\end{center}
Consider the following game state \imgRef{fig:connectFourExample} at which the starting player is at turn.
\begin{center}
\includegraphics[width=0.5\textwidth]{connectFourExample}
\captionof{figure}{example game state}
\label{fig:connectFourExample}
\end{center}
This state's boolean array \(a\) is:
\begin{equation}
a =
\text{vec}\left(\left[\begin{matrix}
\mathColor{black}{0} & \mathColor{black}{0} & \mathColor{black}{0} & \mathColor{black}{0} & \mathColor{black}{0} & \mathColor{black}{0} & \mathColor{black}{0}\\
\mathColor{black}{0} & \mathColor{black}{0} & \mathColor{black}{0} & \mathColor{red}{1}     & \mathColor{black}{0} & \mathColor{black}{0} & \mathColor{black}{0}\\
\mathColor{black}{0} & \mathColor{black}{0} & \mathColor{\gold}{0} & \mathColor{\gold}{0} & \mathColor{black}{0} & \mathColor{black}{0} & \mathColor{black}{0}\\
\mathColor{black}{0} & \mathColor{black}{0} & \mathColor{red}{1}     & \mathColor{\gold}{0} & \mathColor{black}{0} & \mathColor{black}{0} & \mathColor{black}{0}\\
\mathColor{black}{0} & \mathColor{black}{0} & \mathColor{red}{1}     & \mathColor{red}{1}     & \mathColor{\gold}{0} & \mathColor{black}{0} & \mathColor{black}{0}\\
\mathColor{black}{0} & \mathColor{black}{0} & \mathColor{red}{1}     & \mathColor{\gold}{0} & \mathColor{\gold}{0} & \mathColor{red}{1}     & \mathColor{black}{0}\\
\end{matrix}
\right]\right)
\frown vec
\left(\left[
\begin{matrix}
\mathColor{black}{0} & \mathColor{black}{0} & \mathColor{black}{0} & \mathColor{black}{0} & \mathColor{black}{0} & \mathColor{black}{0} & \mathColor{black}{0}\\
\mathColor{black}{0} & \mathColor{black}{0} & \mathColor{black}{0} & \mathColor{red}{0}     & \mathColor{black}{0} & \mathColor{black}{0} & \mathColor{black}{0}\\
\mathColor{black}{0} & \mathColor{black}{0} & \mathColor{\gold}{1} & \mathColor{\gold}{1} & \mathColor{black}{0} & \mathColor{black}{0} & \mathColor{black}{0}\\
\mathColor{black}{0} & \mathColor{black}{0} & \mathColor{red}{0}     & \mathColor{\gold}{1} & \mathColor{black}{0} & \mathColor{black}{0} & \mathColor{black}{0}\\
\mathColor{black}{0} & \mathColor{black}{0} & \mathColor{red}{0}     & \mathColor{red}{0}     & \mathColor{\gold}{1} & \mathColor{black}{0} & \mathColor{black}{0}\\
\mathColor{black}{0} & \mathColor{black}{0} & \mathColor{red}{0}     & \mathColor{\gold}{1} & \mathColor{\gold}{1} & \mathColor{red}{0}     & \mathColor{black}{0}\\
%&&& 1 &&&]\\
\end{matrix}\right]\right)\frown \left<1\right>
\end{equation}
Where \(vec\) is the matrix vectorization defined in \sectionref{sec:ref:vectorization} and \(\frown\) is the vector concatination defined in \sectionref{sec:ref:vectorConcatination}.
To the front of this vector, the version of the AI we want to play against is added. If the version is invalid the server will default to the best version.



\subsubsection{Action Selection}
The AI's action is selected by running MCTS simulations and choosing the action with the most evaluations. It is the same algorithm as the deterministic method defined in \sectionref{sec:training:actionSelection}.
\subsubsection{Action Transmission}
The action is stored and transmitted as a four byte integer.

\subsection{Clients}
There are two clients for the AlphaZero system. The first is a python client for DOS, macOS, Linux, etc., and the second was developped for iOS using pythonista.
\subsubsection{Desktop Client}
The Desktop client will allow the player to play against an AI with a slightly better elo-rating than the player's. This is done by creating an account on the data server. The data server will give the client a unique number representing its account. The Client will then proceed to save this number and request the player's elo-rating. Finally, the AI version with the closest but better elo-rating to that of the player is requested. Now the client renders the game board and randomly selects a starting player. The client will tean wait for user inputs and query the server as appropriate, i.e. what player is at turn. When the game is done, the client's log of the game is uploaded to the data server and the player will be shown the appropriate end-of-the-game screen. The client also has the possibility to request game logs and replay games.
This client version uses the python socket library for communication and Tkinter to render the game board.
\subsubsection{iOS Client}\label{sec:iosClient}
The iOS version does the same thing as the Desktop version with a few differences. Firstly, it renders the board using the pythonista scene library. Secondly, it dose not create or store accounts. The player will always play against the best AI version. Thirdly, it does not have the possiblity to replay games. Lastly, the Client will request its actual source code from the web server to allow for easier updating.


\section{Further definitions}
\subsection{Set Exclusion}
Let \(\mathbb A - \mathbb B\) be the set exclusion of \(\mathbb A\) and \(\mathbb B\).
\begin{equation}\label{eq:defs:SetExclusion}
\mathbb A - \mathbb B = \mathbb A \setminus \mathbb B = \left\{x: x\in\mathbb  A ~\text{and}~ x \notin\mathbb B\right\}
\end{equation}
\subsection{Hadamard product}\label{sec:hadamerd_product}
Let \(A\) and \(B\) be 2 \(m \times n\) matrices. For all \(i \in [1, m]\) and \(j \in [1, n]\)
the hadamard product \(A \circ B\) is defined as:
\begin{equation} \label{eq:defs:Hadamard product}
\left(A \circ B\right)_{ij} = A_{ij} \cdot B_{ij}
\end{equation}
For example consider the following \(2 \times 3\) matrices:
\[
\left[
\begin{array}{ll}
a_{11} & a_{12} \\
a_{21} & a_{22} \\
a_{31} & b_{32} \\
\end{array}
\right] \circ 
\left[
\begin{array}{ll}
b_{11} & b_{12} \\
b_{21} & b_{22} \\
b_{31} & b_{32} \\
\end{array}
\right] = 
\left[
\begin{array}{ll}
a_{11}b_{11} & a_{12}b_{12} \\
a_{21}b_{21} & a_{22}b_{22} \\
a_{31}b_{31} & a_{32}b_{32} \\
\end{array}
\right]
\]

\subsection{Inner Product}
\subsubsection{Matrices}
Let \(A\in \mathbb{R}^{m \times n}\) and \(B\in \mathbb{R}^{m \times n}\) be two \(n \times m\) matrices. \\Let their Inner Product \(\left<A, B\right>_I:\; \mathbb{R}^{m \times n},~\mathbb{R}^{m \times n} \to \mathbb{R}\) be defined as:
\begin{equation} \label{eq:defs:Inner_product}
\left<A, B\right>_I = \sum_{i=1}^{m}\sum_{j=1}^{n} A_{ij}B_{ij} = \sum_{i=1}^{m}\sum_{j=1}^{n} (A \circ B)_{ij}
\end{equation}
For example consider the following \(2 \times 3\) matrices:
\[
\left<
\begin{bmatrix}
a_{11} & a_{12} \\
a_{21} & a_{22} \\
a_{31} & b_{32} \\
\end{bmatrix}
\begin{matrix} \\\\,\end{matrix}~
\begin{bmatrix}
b_{11} & b_{12} \\
b_{21} & b_{22} \\
b_{31} & b_{32} \\
\end{bmatrix}\right>_I
= \;
\begin{array}{ll}
     a_{11}b_{11} + a_{12}b_{12} + a_{21}b_{21}\\
+~a_{22}b_{22} + a_{31}b_{31} + a_{32}b_{32} 
\end{array}
\]

\subsubsection{\(n\)-dimensional Tensors}
Let \(A \in \mathbb{R}^{m \times \dots}\) and \(B \in \mathbb{R}^{m \times \dots}\) be two \(n\) dimensional \(m \times \dots\) tensors with \(n > 2\)\\
Let the Inner product \(\left<A, B\right>_I : \mathbb{R}^{m \times \dots}, \mathbb{R}^{m \times \dots} \to \mathbb{R}\) be defined as:
\begin{equation}\label{eq:defs:Inner_product_3d}
\left<A, B\right>_I = \sum_{i=0}^{m} \left<A_{i}, B_{i}\right>_I
\end{equation}

%sidewaysfigure
\begin{sidewaysfigure}[ht]
    \centering
	\captionsetup{width=.9\linewidth}
    \includesvg[width=\columnwidth]{images/MCTS-steps.svg}
		\caption[width=0.7\columnwidth]{MCTS simulation steps. In this diagram, the numbers in the node represent \(Q\) and the number on the arrow is \(P\). The red nodes are leaf nodes and the green one is the leaf node \(n_L\). During the \textbf{selection} phase, \(\sigma\) is used to find successive nodes until the node \(n_L\) is reached. This is shown with the arrows. During the \textbf{expansion} phase, new nodes and edges are added for all possible legal actions at the node \(n_L\). The \textbf{evaluation} phase gives the new nodes the following values \(Q = 0\) and \(P = \pi_a\). The value of the leaf \(v\) is then used during the \textbf{backfill} phase to update the \(Q\)'s of all nodes traversed during selection.}
		\source{modified from \url{https://en.wikipedia.org/wiki/File:MCTS-steps.svg}\\File available under \href{https://creativecommons.org/licenses/by-sa/4.0/deed.en}{Creative Commons Attribution-Share Alike 4.0 International} at \url{https://wandhoven.ddns.net/edu/AlphaZeroTheory/images/MCTS-steps.svg}}
	\label{MCTS-simulation}
\end{sidewaysfigure}

\subsection{Submatrix}
\label{sec:Ref:submatrix}
Let \(m = m_{ijk}\) be an \(m\times n\times o\) dimensional tensor.\\
Let \(<>_{S\,x\times y,\,ab}\) be the matrix slicing opperator. \(x\times y\) is the size of the submatrix the operation should output. \(ab\) is the top left position of the submatrix within the outer matrix.  For the operation to be defined, the following must be true: \(x \in \mathbb [0, m[\), \(y \in \mathbb [0, n[\), \(a  \in [1, m-x]\) and \(b \in [1, n-y]\). The submatrix \(<m>_{S\,x\times y,\,ab}\) is defined as:
\begin{equation}
<m>_{S\,x\times y,\,ab} = 
\left[
\begin{matrix}
[m_{ab1},\dots,m_{abo}] & \dots & [m_{(a+x)b1},\dots,m_{(a+x)bo}]\\
\vdots &  \ddots & \vdots\\
[m_{a(b+y)1},\dots,m_{abo}] & \dots & [m_{(a+x)b1},\dots,m_{(a+x)(b+y)o}]\\
\end{matrix}\right]
\end{equation}

\subsection{Vectorization}  \label{sec:ref:vectorization}
The vectorization of an \(m \times n\) matrix \(A\), denoted \(\text{vec}(A)\), is the \(mn \times 1\) column vector obtained by stacking the columns of the matrix A on top of one another: 
\begin{equation}\label{eq:ref:vectorization}
\text{vec}(A) = \left[
A_{11}\,\dots\,A_{1m}\,A_{21}\,\dots \,A{2m}\,\dots\,A_{n1}\,\dots\,A{nm}
\right]^T
\end{equation}

Taken verbatim from:\cite{vectorization_2021}\\
For example, the \(3\times 2\) matrix \(A=\left[
\begin{matrix}
a & b & c\\
d & e & f\\
\end{matrix}
\right]\) vectorizes to
\[
\text{vec}(A) = \left(
\begin{matrix}
a\\
b\\
c\\
d\\
e\\
f\\
\end{matrix}
\right)
\]
\subsubsection{Tensors}\label{sec:ref:vectorization:tensor}
Let \(T \in \mathbb R^{n \times\dots}\) be a n-dimensional Tensor.
Let \(vec(T)\) be defined as:
\begin{equation}
vec(T) = T_0 \frown T_1 \frown \dots \frown T_n
\end{equation}
Where \(\frown\) is defined defined in \sectionref{sec:ref:vectorConcatination}. For those who are familiar with python's numpy library, \(vec\) is equivilent to numpy.flatten.

\subsection{Vector Concatination}\label{sec:ref:vectorConcatination}
Vector Concatination of two vectors \(v\) and \(u\) of dimensions \(n_v\) and  \(n_u\), denoted \(v\frown u\), is the \(n_v + n_u\) dimensional vector obtained by placing both vectors one on top of the other.
\begin{equation} \label{eq:ref:vecConcatination}
v\frown u = \left(
\begin{matrix}
v_1\\
\vdots\\
v_{n_v}\\
u_1\\
\vdots\\
u_{n_u}\\
\end{matrix}
\right)
\end{equation}

\subsection{Future Assignment Operator}
\label{sec:assignmentOperator}
The future assignemtn operator, denoted \(\Rightarrow\), is in the explenation of iterative algorythems to represent assigning an value to a variable for the next iteration. The operation implies the existance of an index \(i\). Let \(x\) represent any variable and \(v\) be any possible value of \(x\). On the \(i\)\textsuperscript{th} use of the future assignment operator on \(x\). 
\begin{equation}
v \Rightarrow x
\end{equation}
is equivilent to
\begin{equation}
v = x_{i+1}
\end{equation}
Furthermore any variable refered to, that the uses the future assignment operator, will automatically refer to the last set one. This means that if an algorythem uses \(x\) and the initial value of \(x = x_0 = z\), where \(z\) is a possible value of  \(x\). Than that algorythem uses this operation on \(x\), \(v \Rightarrow x\). Than from that  point forward in the logical operations all referances to \(x = x_1 = v\).\\
The reason for this is that using indecies in the MCTS is more confusing than helpfull.
\subsection{Sets}
Let
\begin{equation}\label{set:Rab}
\mathbb R_{a, b} = \{x \in \mathbb R~:~ a\leq x \leq b\}
\end{equation}

\bibliographystyle{plain}
\bibliography{bibliography}

\section*{Acknowledgements}
\begin{itemize}
\item Leonie Scheck, Frederik Ott, Nico Steiner, Wolfgang Wandhoven for aiding in evaluating the AI against human players and providing feedback.
\end{itemize}

%\newpage\section{Code}
%\subsection{AlphaZeroPytorch}
%\subsubsection{AlphaZeroPytorch.h}							\incFile{C++}{AlphaZeroPytorch/AlphaZeroPytorch.h}
%\subsubsection{AlphaZeroPytorch.cpp}						\incFile{C++}{AlphaZeroPytorch/AlphaZeroPytorch.cpp}
%\subsubsection{cmake.sh}												\incFile{sh}{AlphaZeroPytorch/cmake.sh}
%\subsubsection{CMakeLists.txt}										\incFile{python}{AlphaZeroPytorch/CMakeLists.txt}
%\subsubsection{convertToJceFormat.cpp}					\incFile{C++}{AlphaZeroPytorch/convertToJceFormat.cpp}
%%\incFile{C++}{AlphaZeroPytorch/detect_cuda_compute_capabilities.cpp}
%%\incFile{C++}{AlphaZeroPytorch/detect_cuda_version.cc}
%\subsubsection{doEloRaiting.cpp}									\incFile{C++}{AlphaZeroPytorch/doEloRaiting.cpp}
%\subsubsection{makeFiles.hpp}										\incFile{C++}{AlphaZeroPytorch/makeFiles.hpp}
%\subsubsection{Replay.cpp}											\incFile{C++}{AlphaZeroPytorch/Replay.cpp}
%\subsubsection{runServer.cpp}										\incFile{C++}{AlphaZeroPytorch/runServer.cpp}
%\subsubsection{showLoss.cpp}										\incFile{C++}{AlphaZeroPytorch/showLoss.cpp}
%\subsubsection{test.cpp}												\incFile{C++}{AlphaZeroPytorch/test.cpp}
%\subsubsection{include}													% AlphaZeroPytorch/include
%\paragraph{config.hpp}													\incFile{C++}{AlphaZeroPytorch/include/config.hpp} \label{code:config}
%\paragraph{config.cpp}													\incFile{C++}{AlphaZeroPytorch/include/config.cpp}
%\paragraph{io.hpp}															\incFile{C++}{AlphaZeroPytorch/include/io.hpp}
%\paragraph{log.hpp}														\incFile{C++}{AlphaZeroPytorch/include/log.hpp}
%\paragraph{log.cpp}														\incFile{C++}{AlphaZeroPytorch/include/log.cpp}
%\paragraph{timer.hpp}													\incFile{C++}{AlphaZeroPytorch/include/timer.hpp}
%\paragraph{ai}																	% AlphaZeroPytorch/include/ai
%\subparagraph{agent.hpp}												\incFile{C++}{AlphaZeroPytorch/include/ai/agent.hpp}
%\subparagraph{agent.cpp}												\incFile{C++}{AlphaZeroPytorch/include/ai/agent.cpp}
%\subparagraph{MCTS.hpp}												\incFile{C++}{AlphaZeroPytorch/include/ai/MCTS.hpp}
%\subparagraph{MCTS.cpp}												\incFile{C++}{AlphaZeroPytorch/include/ai/MCTS.cpp}
%\subparagraph{memory.hpp}											\incFile{C++}{AlphaZeroPytorch/include/ai/memory.hpp}
%\subparagraph{memory.cpp}											\incFile{C++}{AlphaZeroPytorch/include/ai/memory.cpp}
%\subparagraph{model.hpp}											\incFile{C++}{AlphaZeroPytorch/include/ai/model.hpp}
%\subparagraph{modelSynchronizer.hpp}						\incFile{C++}{AlphaZeroPytorch/include/ai/modelSynchronizer.hpp}
%\subparagraph{modelWorker.hpp}								\incFile{C++}{AlphaZeroPytorch/include/ai/modelWorker.hpp}
%\subparagraph{modelWorker.cpp}									\incFile{C++}{AlphaZeroPytorch/include/ai/modelWorker.cpp}
%\subparagraph{playGame.hpp}										\incFile{C++}{AlphaZeroPytorch/include/ai/playGame.hpp}
%\subparagraph{playGame.cpp}										\incFile{C++}{AlphaZeroPytorch/include/ai/playGame.cpp}
%\subparagraph{utils.hpp}												\incFile{C++}{AlphaZeroPytorch/include/ai/utils.hpp}
%\paragraph{game}															% AlphaZeroPytorch/include/game
%\subparagraph{game.hpp}												\incFile{C++}{AlphaZeroPytorch/include/game/game.hpp}
%\subparagraph{game.cpp}												\incFile{C++}{AlphaZeroPytorch/include/game/game.cpp}
%\paragraph{jce}																% AlphaZeroPytorch/include/jce
%\subparagraph{load.hpp}												\incFile{C++}{AlphaZeroPytorch/include/jce/load.hpp}
%\subparagraph{save.hpp}												\incFile{C++}{AlphaZeroPytorch/include/jce/save.hpp}
%\subparagraph{string.hpp}												\incFile{C++}{AlphaZeroPytorch/include/jce/string.hpp}
%\subparagraph{vector.hpp}											\incFile{C++}{AlphaZeroPytorch/include/jce/vector.hpp}
%\paragraph{server}															% AlphaZeroPytorch/include/server
%\subparagraph{eloClient.hpp}										\incFile{C++}{AlphaZeroPytorch/include/Server/eloClient.hpp}
%\subparagraph{server.hpp}												\incFile{C++}{AlphaZeroPytorch/include/Server/server.hpp}
%\subparagraph{server.cpp}												\incFile{C++}{AlphaZeroPytorch/include/Server/server.cpp}
%\paragraph{test}																% AlphaZeroPytorch/include/test
%\subparagraph{testSuit.hpp}											\incFile{C++}{AlphaZeroPytorch/include/test/testSuit.hpp}	
%\subparagraph{testSuit.cpp}											\incFile{C++}{AlphaZeroPytorch/include/test/testSuit.cpp}
%\subparagraph{testUtils.hpp}											\incFile{C++}{AlphaZeroPytorch/include/test/testUtils.hpp}
%
%\subsection{Clients}														% Clients/ConsoleClient
%\subsubsection{ConsoleClient}
%\paragraph{ConsoleClient.h}											\incFile{C++}{Clients/ConsoleClient/ConsoleClient.h}
%\paragraph{ConsoleClient.cpp}										\incFile{C++}{Clients/ConsoleClient/ConsoleClient.cpp}
%\paragraph{include}														% clients/consoleClient/include
%\subparagraph{agent.hpp}												\incFile{C++}{Clients/ConsoleClient/include/agent.hpp}
%\subparagraph{game.hpp}												\incFile{C++}{Clients/ConsoleClient/include/game.hpp}
%\subparagraph{modifications.hpp}								\incFile{C++}{Clients/ConsoleClient/include/modifications.hpp}
%\paragraph{scr}																% Clients/ConsoleClient/src
%\subparagraph{game.cpp}												\incFile{C++}{Clients/ConsoleClient/scr/game.cpp}
%\subsubsection{iosClient}												% Clients/iosClient
%\paragraph{caller.py}														\incFile{python}{Clients/iosClient/caller.py}
%\paragraph{connect4IOS.py}											\incFile{python}{Clients/iosClient/connect4IOS.py}
%\subsubsection{pyClient}												% Clients/pyClient
%\paragraph{Client.py}														\incFile{python}{Clients/pyClient/Client.py}
%\paragraph{game.py}														\incFile{python}{Clients/pyClient/game.py}
%\paragraph{gameSaver.py}												\incFile{python}{Clients/pyClient/gameSaver.py}
%\paragraph{GUI.py}															\incFile{python}{Clients/pyClient/GUI.py}
%\paragraph{main.py}														\incFile{python}{Clients/pyClient/main.py}
%\paragraph{test.py}														\incFile{python}{Clients/pyClient/test.py}
%\paragraph{winStates.json}											\incFile{python}{Clients/pyClient/winStates.json}
%
%\subsection{elo}																% /elo
%\subsubsection{agent.py}												\incFile{python}{elo/agent.py}
%\subsubsection{renderElo.py}										\incFile{python}{elo/renderElo.py}
%\subsubsection{score.py}												\incFile{python}{elo/score.py}
%\subsubsection{server.py}												\incFile{python}{elo/server.py}
%
%\subsection{game}															% games/connect4
%\subsubsection{connect4}
%\paragraph{config.hpp}													\incFile{C++}{games/connect4/config.hpp}
%\paragraph{game.hpp}													\incFile{C++}{games/connect4/game.hpp}
%\paragraph{game.cpp}													\incFile{C++}{games/connect4/game.cpp}
%\subsubsection{othello}													% games/othello
%\paragraph{config.hpp}													\incFile{C++}{games/othello/config.hpp}
%\paragraph{game.hpp}													\incFile{C++}{games/othello/game.hpp}
%\paragraph{game.cpp}													\incFile{C++}{games/othello/game.cpp}
%
%% /
%\subsection{CMakeLists.txt}											\incFile{python}{CMakeLists.txt}
%\subsection{README.md}												\incFile{python}{README.md}
%\subsection{serch.sh}														\incFile{sh}{serch.sh}
%
%\section{Demos}
%\subsection{Matura-AlphaZero-demos}
%\subsubsection{CMakeLists.txt}										\incDemo{python}{CMakeLists.txt}
%%\subsubsection{LICENSE}											\incDemo{python}{LICENSE}
%\subsubsection{main.cpp}												\incDemo{c++}{main.cpp}
%\subsubsection{README.md}										\incDemo{python}{README.md}
%\subsubsection{supervised.cpp}									\incDemo{c++}{supervised.cpp}
%\subsubsection{include}													% demo/include
%\paragraph{point.hpp}													\incDemo{c++}{include/point.hpp}
%\paragraph{utils.hpp}														\incDemo{c++}{include/utils.hpp}
%\subsubsection{scr}														% demo/scr
%\paragraph{point.cpp}													\incDemo{c++}{scr/point.cpp}
%\subsubsection{supervised}											% demo/supervised
%\paragraph{include}														% demo/supervised/include
%\subparagraph{config.hpp}											\incDemo{c++}{supervised/include/config.hpp}
%\subparagraph{model.hpp}											\incDemo{c++}{supervised/include/model.hpp}
%\paragraph{src}																% demo/supervised/src
%\subparagraph{model.cpp}												\incDemo{c++}{supervised/src/model.cpp}
%\subsubsection{unsupervised}										% demo/unsupervised
%\paragraph{include}														% demo/unsupervised/include
%\subparagraph{cluster.hpp}											\incDemo{c++}{unsupervised/include/cluster.hpp}
%\subparagraph{config.hpp}											\incDemo{c++}{unsupervised/include/config.hpp}
%\subparagraph{group.hpp}												\incDemo{c++}{unsupervised/include/group.hpp}
%\paragraph{src}																% demo/unsupervised/src
%%\subparagraph{cluster.cpp}											\incDemo{c++}{unsupervised/src/cluster.cpp}
%%\subparagraph{group.cpp}												\incDemo{c++}{unsupervised/src/group.cpp}
%
\section{Examples}
\subsection{Mcts}\label{mctsExampleFullData}
The full set of data as seen by the computer in the mcts example \imgRef{fig:mcts:example}. The node sets are as follows defined by the amount of edged leaving each node.
\begin{align}
 \mathbb M 										&= \{n_1, n_2, n_3, n_4, n_5, n_6, n_7, n_8\}\\
 \mathbb M_{leaf} 						&= \{n_3, n_5, n_6, n_7, n_8\}\\
 \mathbb M_{expanded} 	&= \{n_1, n_2, n_4\}
\end{align}
The edge sets for every node are as follows.
\begin{align}
 \mathbb E(n_1) &= \{e_1, e_2, e_3\}\\
 \mathbb E(n_2) &= \{e_4, e_5\}\\
 \mathbb E(n_3) &= \emptyset\\
 \mathbb E(n_4) &= \{e_6, e_7, e_8\}\\
 \mathbb E(n_5) &= \emptyset\\
 \mathbb E(n_6) &= \emptyset\\
 \mathbb E(n_7) &= \emptyset\\
 \mathbb E(n_8) &= \emptyset
\end{align}
The edge by action and node function \(\mathcal E\) looks af follows:
\begin{align}
   \mathcal E(n_1, A_1) &= e_1\\
	\mathcal E(n_1, A_2) &= e_2\\
	\mathcal E(n_1, A_3) &= e_3\\
	\mathcal E(n_2, A_4) &= e_4\\
	\mathcal E(n_2, A_5) &= e_5\\
	\mathcal E(n_4, A_6) &= e_6\\
	\mathcal E(n_4, A_7) &= e_7\\
	\mathcal E(n_4, A_8) &= e_8
\end{align}
The node an edge points from function \(\mathcal N_{from}\) looks af follows:
\begin{align}
	\mathcal N_{from} (e_1) &= n_1\\
	\mathcal N_{from} (e_2) &= n_1\\
	\mathcal N_{from} (e_3) &= n_1\\
	\mathcal N_{from} (e_4) &= n_2\\
	\mathcal N_{from} (e_5) &= n_2\\
	\mathcal N_{from} (e_6) &= n_4\\
	\mathcal N_{from} (e_7) &= n_4\\
	\mathcal N_{from} (e_8) &= n_4
\end{align}
The node an edge points to function \(\mathcal N_{to}\) looks af follows:
\begin{align}
	\mathcal N_{to} (e_1) &= n_3\\
	\mathcal N_{to} (e_2) &= n_2\\
	\mathcal N_{to} (e_3) &= n_4\\
	\mathcal N_{to} (e_4) &= n_5\\
	\mathcal N_{to} (e_5) &= n_6\\
	\mathcal N_{to} (e_6) &= n_6\\
	\mathcal N_{to} (e_7) &= n_7\\
	\mathcal N_{to} (e_8) &= n_8
\end{align}

\section{Proofs}
\subsection{tanh derivative} \label{sec:proof:tanhd/dx}
\(tanh~:~\mathbb R~\to~\mathbb R\) is defined as:
\begin{equation}
tanh(x) = \frac{e^x-e^{-x}}{e^x+e^{-x}}
\end{equation}
\begin{align}
\begin{split}
\frac{d}{dx}\left[ \frac{e^x-e^{-x}}{e^x+e^{-x}}\right] &= \frac{ 
													(e^x+e^{-x})\frac{d}{dx}\left[e^x-e^{-x}\right]
												- (e^x-e^{-x}) \frac{d}{dx}\left[e^x+e^{-x}\right]}
											{(e^x+e^{-x})^2}\\
&= \frac{(e^x+e^{-x})^2 - (e^x-e^{-x})^2}{(e^x+e^{-x})^2}\\
&= \frac{e^{2x} + 2e^xe^{-x} + e^{-2x} - e^{2x} + 2e^xe^{-x} -e^{-x}}{(e^x+e^{-2x})^2}\\
&= \frac{4}{(e^x+e^{-x})^2}
\end{split}
\label{eq:tanhDerivativeEnd}
\end{align}
because
\begin{equation}\label{eq:sech}
sech(x) := \frac{2}{e^x+e^{-x}}
\end{equation}
using (\ref{eq:tanhDerivativeEnd}) and (\ref{eq:sech}) we can see, that
\begin{equation}
\frac{d}{dx}\left[tanh(x)\right] = sech(x)^2
\end{equation}

\subsection{Softmax Derivative}
The Softmax \(s~:~\mathbb R^n~\to~\mathbb R^n\), \(n\in\mathbb N\), is defined as
\begin{equation} \label{eq:proof:softmax}
s(v)_i := \frac{e^{v_i}}{\sum_{j \in v} e^j}
\end{equation}
for all \(i \in [1,n]\). Due to the multidimensional natur of this function we are taking the derivative \(\frac{d}{dv_j}s(v)_i\), with \(i \in [1,n]\) and \(j \in [1,n]\). 
\subparagraph*{case \(i \neq j\)}
\begin{align}
\begin{split}
\frac{d}{dv_j}\left[\frac{e^{v_i}}{\sum_{a \in v} e^a}\right] &= -\frac{\frac{d}{dx}\left[\sum_{a \in v} e^a\right]e^{v_i}}{(\sum_{a \in v} e^a)^2}\\
&= -\frac{e^{v_j} e^{v_i}}{(\sum_{a \in v} e^a)^2}\\
&= -s(v)_j s(v)_i
\end{split}
\end{align}
\subparagraph*{case \(i = j\)}
\begin{align}
\begin{split}
\frac{d}{dv_j}\left[\frac{e^{v_i}}{\sum_{a \in v} e^a}\right] 
&= \frac{\frac{d}{dx}\left[e^{v_i}\right]\sum_{a \in v} e^a - e^{v_i}\frac{d}{dx}\left[\sum_{a \in v} e^a\right]}{(\sum_{a \in v} e^a)^2}\\
&= \frac{e^{v_i}\sum_{a \in v} e^a - e^{v_i}e^{v_j}}{(\sum_{a \in v} e^a)^2}\\
&= s(v)_i - s(v)_i s(v)_j\\
&= s(v)_i (1 - s(v)_j)
\end{split}
\end{align}

\end{document}
